{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as  transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                      transforms.Grayscale(num_output_channels=1),\n",
    "                      transforms.RandomHorizontalFlip(),\n",
    "                      transforms.Resize((28,28)),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.5,),(0.5,))\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset (update the path as needed)\n",
    "data_dir = \"C:/Swetha/ASEB/Infosys_internship/brain_tumor_dataset\"\n",
    "\n",
    "# Load the dataset with transformations\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Paths to the pre-split datasets (update the paths as needed)\n",
    "train_dir = \"C:/Swetha/ASEB/Infosys_internship/brain_tumor_dataset/Training\"\n",
    "test_dir = \"C:/Swetha/ASEB/Infosys_internship/brain_tumor_dataset/Testing\"\n",
    "\n",
    "\n",
    "# Load the pre-split datasets with transformations\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Image size:\", images.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MLP,self).__init__()\n",
    "    self.fc1 = nn.Linear(28*28,128)\n",
    "    self.fc2 = nn.Linear(128,4)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x.view(-1,28*28)\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5601346735847729 , Accuracy: 79.39425770308124\n",
      "Epoch 2/10, Loss: 0.5363769977452368 , Accuracy: 79.74439775910363\n",
      "Epoch 3/10, Loss: 0.5186508834029043 , Accuracy: 80.89985994397759\n",
      "Epoch 4/10, Loss: 0.49688637980868694 , Accuracy: 81.53011204481793\n",
      "Epoch 5/10, Loss: 0.4848660554466301 , Accuracy: 82.28291316526611\n",
      "Epoch 6/10, Loss: 0.4709683665017176 , Accuracy: 82.44047619047619\n",
      "Epoch 7/10, Loss: 0.45650598090454186 , Accuracy: 83.17577030812325\n",
      "Epoch 8/10, Loss: 0.42984824923163684 , Accuracy: 83.84103641456582\n",
      "Epoch 9/10, Loss: 0.4223604968140245 , Accuracy: 83.98109243697479\n",
      "Epoch 10/10, Loss: 0.4226997289910663 , Accuracy: 83.66596638655462\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  running_loss = 0.0\n",
    "  correct =0\n",
    "  total =0\n",
    "  for inputs, labels in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs ,labels )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    _,predicted = torch.max(outputs,1)\n",
    "    total +=labels.size(0)\n",
    "    correct +=(predicted ==labels).sum().item()\n",
    "\n",
    "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)} , Accuracy: {100*correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate on Test Data\n",
    "model.eval()\n",
    "correct =0\n",
    "total =0\n",
    "act_labels =torch.tensor([])\n",
    "model_predicted =torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "  for inputs, labels in test_loader:\n",
    "    outputs =model(inputs)\n",
    "    act_labels = torch.cat((act_labels ,labels ) ,dim =0)\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "    model_predicted = torch.cat((model_predicted ,predicted),dim=0 )\n",
    "    total +=labels.size(0)\n",
    "    correct += (predicted ==labels).sum().item()\n",
    "print(f\"Test Accuracy: {100*correct/total:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infosys_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
